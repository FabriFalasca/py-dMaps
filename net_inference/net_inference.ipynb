{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# importing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to signals\n",
    "path = './timeseries.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domains ids\n",
    "ids = ['0', '20', '22', '35', '38', '54', '61', '64', '6', '70', '74', '75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the network inference\n",
    "# Tau max\n",
    "tau_max = 12\n",
    "# FDR rate\n",
    "q = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the edge inference. Useful later\n",
    "def edge_inference(corr_matrix,cov_matrix,bartlett_std_matrix,domains_pair):\n",
    "    \n",
    "    # input: (a) correlograms\n",
    "    #        (b) covariances\n",
    "    #        (c) bartlett standard deviations\n",
    "    #        (d) all pairs of domains (same order of correlograms)\n",
    "    \n",
    "    # output list of edges.\n",
    "    #        Each edge have the following format\n",
    "    #        [from domain a, to domain b, tau_min, tau_max, tau*, r*, weight]\n",
    "    #        where \n",
    "    \n",
    "    #       r* is the max significant correlation\n",
    "    #       tau* is the lag correspondent to r*\n",
    "    \n",
    "    #       [tau_min,tau_max] defines the range of lags associated with significant correlations\n",
    "    #       in the interval [r*-bartlett std(r*),r*+bartlett std(r*)]\n",
    "    \n",
    "    #       weight is the link weight: covariance at tau*\n",
    "    \n",
    "    \n",
    "    # Define an array with the lags\n",
    "    lags = np.arange(-tau_max,tau_max+1,1)\n",
    "    \n",
    "    # initialize the array that will hold the edges\n",
    "    edges = []\n",
    "    \n",
    "    for i in range(len(corr_matrix)):\n",
    "        \n",
    "        # If the correlogram is all zeros -> no signifinant correlations -> skip it\n",
    "        if np.sum(np.abs(corr_matrix[i])) == 0:\n",
    "        \n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            # Correlograms of link domains_pair[i]\n",
    "            correlogram = corr_matrix[i]\n",
    "            # Covariances of link domains_pair[i]\n",
    "            covariance = cov_matrix[i]\n",
    "            # Bartletts standard deviations of the correlogram relative to domains_pair[i]\n",
    "            bartlett_sigmas = bartlett_std_matrix[i]\n",
    "        \n",
    "            # Compute the max in absolute value\n",
    "            max_corr = max(correlogram,key=abs)\n",
    "            # Find the correspondent bartlett std\n",
    "            std_max = bartlett_sigmas[correlogram==max_corr][0]\n",
    "            # Keep only the values that are inside 1 sigma of the max\n",
    "            constraints = np.logical_and(correlogram<=max_corr+std_max, correlogram>=max_corr-std_max)\n",
    "            # Find the lags associated with the correlations respecting correlogram\n",
    "            lags_connection = lags[constraints]\n",
    "            # Min and max lag \n",
    "            lag_min = lags_connection[0]\n",
    "            lag_max = lags_connection[-1]\n",
    "            # Lag tau* correspondent to r*\n",
    "            lag_star = lags[correlogram==max_corr][0]\n",
    "            # Weight: covariance at Lag tau*\n",
    "            weight = covariance[correlogram==max_corr][0]\n",
    "            \n",
    "            edge = [domains_pair[i][0],domains_pair[i][1],lag_min,lag_max,lag_star,max_corr,weight]\n",
    "            \n",
    "        edges.append(edge)    \n",
    "        \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_inference_FDR(path,tau_max,q):\n",
    "        # Inputs: \n",
    "    # path to time series\n",
    "    # tau_max: it defines the lag range R \\in [-tau_max,tau_max]\n",
    "    # q: false discovery rate\n",
    "    \n",
    "    # Outputs:\n",
    "    # list with all connections\n",
    "    # list with all strengths\n",
    "    \n",
    "    # import the time series\n",
    "    ts = np.loadtxt(path)\n",
    "    \n",
    "    # Number of signals\n",
    "    N = len(ts)\n",
    "    \n",
    "    # time series have zero mean but NOT 1 sigma\n",
    "    # We standardize the ts for the correlation analysis\n",
    "    normed_ts = (ts.T/np.std(ts,axis=1)).T\n",
    "    \n",
    "    ## Compute the correlograms for each unique pair\n",
    "    correlograms = []\n",
    "    for i in range(N):\n",
    "        ts1 = normed_ts[i]\n",
    "        for j in range(i+1,N):\n",
    "            ts2 = normed_ts[j]\n",
    "            correlograms.append(get_correlogram(ts1,ts2,tau_max,normed=True))\n",
    "\n",
    "    correlograms = np.asarray(correlograms)\n",
    "    \n",
    "    ## Compute the lag covariances for each unique pair\n",
    "    covariances = []\n",
    "    for i in range(N):\n",
    "        ts1 = ts[i]\n",
    "        for j in range(i+1,N):\n",
    "            ts2 = ts[j]\n",
    "            covariances.append(get_covariances(ts1,ts2,tau_max,normed=True))\n",
    "\n",
    "    covariances = np.asarray(covariances)\n",
    "    \n",
    "    ## We want the Bartlett's variance for each unique pair of time series\n",
    "    bartlett = np.zeros([int(N*(N-1)/2),int(2*tau_max+1)])\n",
    "    k1 = 0\n",
    "    for i in range(N):\n",
    "        ts1 = normed_ts[i]\n",
    "        for j in range(i+1,N):\n",
    "            ts2 = normed_ts[j]\n",
    "            k2 = 0\n",
    "            for tau in np.arange(-tau_max,tau_max+1,1):\n",
    "                bartlett[k1,k2] = bartlett_variance_tau(ts1,ts2,tau,normed=True)\n",
    "                k2 += 1\n",
    "            k1 += 1    \n",
    "            \n",
    "    ## Compute the z score for every correlation\n",
    "\n",
    "    # By taking the absolute value of the correlations is like doing a 2 - tailed t-test\n",
    "    abs_correlograms = np.abs(correlograms)\n",
    "    bartlett_std = np.sqrt(bartlett)\n",
    "    z_scores = abs_correlograms/bartlett_std        \n",
    "    \n",
    "    # Compute all p-values for each connection \n",
    "    p_vals = np.zeros([int(N*(N-1)/2),int(2*tau_max+1)])\n",
    "\n",
    "    for i in range(np.shape(z_scores)[0]):\n",
    "        for j in range(np.shape(z_scores)[1]):\n",
    "            ##one sided t-test\n",
    "            p_vals[i,j] = 1 - scipy.stats.norm(0,1).cdf(z_scores[i,j]);\n",
    "            \n",
    "    # Compute p_min\n",
    "\n",
    "    # Step (a)\n",
    "    # flatten all p_values and sort them in descending order\n",
    "    sorted_p_vals = sorted(p_vals.flatten())\n",
    "    sorted_p_vals = np.asarray(sorted_p_vals)\n",
    "    # How many p-values\n",
    "    n_p_vals = len(sorted_p_vals)\n",
    "    # Step (b)\n",
    "    # Define the line for computing the p_min\n",
    "    fdr_ratio = (q/n_p_vals)*np.arange(1,n_p_vals+1)\n",
    "    \n",
    "    ## Get p_min\n",
    "    # (a) take the difference between fdr_ratio[i] and sorted_p_vals[i]\n",
    "    difference = fdr_ratio-sorted_p_vals\n",
    "    # (b) take only the positive entries\n",
    "    positive_vals = sorted_p_vals[difference>0]\n",
    "    # (c) p_min is the last entry\n",
    "    p_min = positive_vals[-1]\n",
    "    \n",
    "    ## pairs is an array with all possible pairs\n",
    "    # It has the same order of the arrays correlations or covariances\n",
    "\n",
    "    # E.g., correlations[0] is the correlogram between the pair of domains in pairs[0]\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(N):\n",
    "        id1 = ids[i]\n",
    "        for j in range(i+1,N):\n",
    "            id2 = ids[j]\n",
    "            pairs.append([id1,id2])\n",
    "\n",
    "    pairs = np.array(pairs) \n",
    "    \n",
    "    ## We set to zero all the correlations with p value > p_min\n",
    "    correlograms[p_vals>p_min] = 0\n",
    "    \n",
    "    ## Compute the network\n",
    "    network = edge_inference(correlograms,covariances,bartlett_std,pairs)\n",
    "    \n",
    "    # Compute the strengths\n",
    "    indices_net = np.array(network)[:,0:2]\n",
    "\n",
    "    strength_list = []\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "\n",
    "        # id considered\n",
    "        id_domain = ids[i]\n",
    "\n",
    "        if len(np.where(indices_net==ids[i])[0]) == 0:\n",
    "\n",
    "            strength_list.append([id_domain,0])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Positions of the index in the net\n",
    "            pos = np.where(indices_net==ids[i])[0]\n",
    "\n",
    "            # Initialize a sublist where we hold the weights associated to each connections\n",
    "            sublists = []\n",
    "\n",
    "            for j in range(len(pos)):\n",
    "\n",
    "                sublists.append(network[pos[j]][6]) # the weight is entry 6 in the network array\n",
    "\n",
    "            strength_list.append([id_domain,np.sum(np.abs(sublists))])\n",
    "            \n",
    "    return network, strength_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list, strength_list = net_inference_FDR(path,tau_max,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20', '22', -1, 4, 1, -0.4761034816622015, -3904.281406571045],\n",
       " ['20', '54', 0, 1, 0, -0.306732434084665, -4794.043300568529],\n",
       " ['20', '61', -10, -3, -6, 0.40693550045107235, 4927.043679229831],\n",
       " ['20', '70', 3, 6, 4, 0.3591438501577918, 4971.025316838217],\n",
       " ['20', '74', -3, 3, 0, -0.658810209804914, -44994.300426043104],\n",
       " ['20', '75', -12, -5, -8, 0.4250177000215324, 9853.407813032309],\n",
       " ['22', '54', -2, 3, 0, 0.34931485882624314, 2406.280549927201],\n",
       " ['22', '61', -8, -1, -6, -0.34119019482853946, -1820.724873194928],\n",
       " ['22', '70', -3, 3, 0, -0.303521308318714, -1851.6277295055056],\n",
       " ['22', '74', -5, 1, -2, 0.5961123306321922, 17943.70983894848],\n",
       " ['22', '75', -12, -7, -11, -0.34838966013687456, -3559.849582490768],\n",
       " ['35', '54', 8, 8, 8, 0.23287601382991405, 1544.3072596373993],\n",
       " ['38', '54', 0, 6, 0, 0.2831577818142795, 2018.6154700520883],\n",
       " ['54', '74', -6, 0, -3, 0.5950186168591978, 34136.35786896345],\n",
       " ['54', '75', -1, 7, 3, 0.3670833257467904, 7148.806543181031],\n",
       " ['61', '74', -1, 9, 5, -0.4177242167604535, -18564.996190533468],\n",
       " ['61', '75', -8, -4, -7, 0.4432796849815473, 6687.523139539875],\n",
       " ['70', '74', -7, 0, -4, -0.3578758686492274, -18182.530860579507],\n",
       " ['74', '75', -9, -8, -9, -0.3295239972528907, -28042.121423125183]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20', 73444.10194228303],\n",
       " ['22', 31486.473980637933],\n",
       " ['35', 1544.3072596373993],\n",
       " ['38', 2018.6154700520883],\n",
       " ['54', 52048.4109923297],\n",
       " ['61', 32000.2878824981],\n",
       " ['70', 25005.18390692323],\n",
       " ['74', 161864.0166081932],\n",
       " ['75', 55291.70850136917]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strength_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
